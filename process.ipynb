{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/lichess_db_puzzle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>RatingDeviation</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>NbPlays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.888765e+06</td>\n",
       "      <td>3.888765e+06</td>\n",
       "      <td>3.888765e+06</td>\n",
       "      <td>3.888765e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.515566e+03</td>\n",
       "      <td>9.027605e+01</td>\n",
       "      <td>8.354723e+01</td>\n",
       "      <td>1.674349e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.432348e+02</td>\n",
       "      <td>4.303419e+01</td>\n",
       "      <td>2.140293e+01</td>\n",
       "      <td>4.459941e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.990000e+02</td>\n",
       "      <td>4.900000e+01</td>\n",
       "      <td>-1.000000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.085000e+03</td>\n",
       "      <td>7.500000e+01</td>\n",
       "      <td>8.200000e+01</td>\n",
       "      <td>9.300000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.478000e+03</td>\n",
       "      <td>7.800000e+01</td>\n",
       "      <td>8.900000e+01</td>\n",
       "      <td>3.710000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.917000e+03</td>\n",
       "      <td>8.800000e+01</td>\n",
       "      <td>9.300000e+01</td>\n",
       "      <td>1.325000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.331000e+03</td>\n",
       "      <td>5.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.007625e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Rating  RatingDeviation    Popularity       NbPlays\n",
       "count  3.888765e+06     3.888765e+06  3.888765e+06  3.888765e+06\n",
       "mean   1.515566e+03     9.027605e+01  8.354723e+01  1.674349e+03\n",
       "std    5.432348e+02     4.303419e+01  2.140293e+01  4.459941e+03\n",
       "min    3.990000e+02     4.900000e+01 -1.000000e+02  0.000000e+00\n",
       "25%    1.085000e+03     7.500000e+01  8.200000e+01  9.300000e+01\n",
       "50%    1.478000e+03     7.800000e+01  8.900000e+01  3.710000e+02\n",
       "75%    1.917000e+03     8.800000e+01  9.300000e+01  1.325000e+03\n",
       "max    3.331000e+03     5.000000e+02  1.000000e+02  1.007625e+06"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df.iloc[:lim,:][['PuzzleId','FEN','Moves']]\n",
    "targets=df.iloc[:lim,:]['Rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(fen):\n",
    "    board = chess.Board(fen)\n",
    "    features = {}\n",
    "\n",
    "    # Material count\n",
    "    piece_values = {chess.PAWN: 1, chess.KNIGHT: 3, chess.BISHOP: 3, chess.ROOK: 5, chess.QUEEN: 9}\n",
    "    features['material_white'] = sum(len(board.pieces(piece, chess.WHITE)) * piece_values[piece] for piece in chess.PIECE_TYPES)\n",
    "    features['material_black'] = sum(len(board.pieces(piece, chess.BLACK)) * piece_values[piece] for piece in chess.PIECE_TYPES)\n",
    "\n",
    "    # Material imbalance\n",
    "    features['material_imbalance'] = features['material_white'] - features['material_black']\n",
    "\n",
    "    # King safety (number of attacking pieces)\n",
    "    features['king_safety_white'] = sum(len(board.attackers(chess.BLACK, square)) for square in board.attackers(chess.BLACK, board.king(chess.WHITE)))\n",
    "    features['king_safety_black'] = sum(len(board.attackers(chess.WHITE, square)) for square in board.attackers(chess.WHITE, board.king(chess.BLACK)))\n",
    "\n",
    "    # Piece activity (mobility)\n",
    "    features['mobility_white'] = sum(len(board.attacks(square)) for square in board.pieces(chess.KNIGHT, chess.WHITE))\n",
    "    features['mobility_black'] = sum(len(board.attacks(square)) for square in board.pieces(chess.KNIGHT, chess.BLACK))\n",
    "\n",
    "    # Pawn structure\n",
    "    features['pawn_white'] = len(board.pieces(chess.PAWN, chess.WHITE))\n",
    "    features['pawn_black'] = len(board.pieces(chess.PAWN, chess.BLACK))\n",
    "    features['doubled_pawns_white'] = len([file for file in range(8) if len(board.pieces(chess.PAWN, chess.WHITE) & chess.SquareSet(chess.BB_FILES[file])) > 1])\n",
    "    features['doubled_pawns_black'] = len([file for file in range(8) if len(board.pieces(chess.PAWN, chess.BLACK) & chess.SquareSet(chess.BB_FILES[file])) > 1])\n",
    "\n",
    "    # Center control\n",
    "    center_squares = [chess.D4, chess.E4, chess.D5, chess.E5]\n",
    "    features['center_control_white'] = sum(1 for square in center_squares if board.piece_at(square) and board.piece_at(square).color == chess.WHITE)\n",
    "    features['center_control_black'] = sum(1 for square in center_squares if board.piece_at(square) and board.piece_at(square).color == chess.BLACK)\n",
    "\n",
    "    # Additional features can be added here...\n",
    "\n",
    "    return pd.Series(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chess\n",
    "\n",
    "def extract_features(fen):\n",
    "    board = chess.Board(fen)\n",
    "    features = {}\n",
    "\n",
    "    # Material count\n",
    "    features['material_white'] = sum(len(board.pieces(piece, chess.WHITE)) for piece in chess.PIECE_TYPES)\n",
    "    features['material_black'] = sum(len(board.pieces(piece, chess.BLACK)) for piece in chess.PIECE_TYPES)\n",
    "\n",
    "    # Material imbalance\n",
    "    features['material_imbalance'] = features['material_white'] - features['material_black']\n",
    "\n",
    "    # King safety\n",
    "    features['king_safety_white'] = sum(len(board.attackers(chess.BLACK, board.king(chess.WHITE))) for _ in range(1))\n",
    "    features['king_safety_black'] = sum(len(board.attackers(chess.WHITE, board.king(chess.BLACK))) for _ in range(1))\n",
    "    # Additional features can be added here...\n",
    "\n",
    "    return pd.Series(features)\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "df_t = df_train['FEN'].apply(extract_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "material_white        4806383\n",
       "material_black        4826493\n",
       "material_imbalance     -20110\n",
       "king_safety_white       29477\n",
       "king_safety_black       33692\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now each feature is a column in your DataFrame\n",
    "df_t.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_t, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 286923.92938682425\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model training\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
